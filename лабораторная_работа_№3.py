# -*- coding: utf-8 -*-
"""Лабораторная работа №3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p0e1GWOXV_yAEfw0-IREUeXWwc6KkTZy
"""

from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import to_categorical

#Загружаем данные
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

#Изображения имеют размер 28x28 и, следовательно, является двухмерными. Поскольку наш персептрон способен считывать только одномерные данные, преобразуем их.
x_train = x_train.reshape(x_train.shape[0], -1) / 255.0
x_test = x_test.reshape(x_test.shape[0], -1) / 255.0
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

#Создаем последовательную модель №1 (Слои будут добавляться один за другим, в порядке их объявления.)
model = Sequential()
#Добавляем слои
model.add(Dense(10, input_dim=784, activation='relu')) #10 нейронов, входной вектор изображения 28х28=784, функция активации relu
# первый слой принимает вектор из 784 чисел и преобразует его в вектор из 10 чисел с использованием линейного преобразования (умножение на матрицу весов и прибавление смещений), затем применяет ReLU.
model.add(Dense(10, activation='softmax')) #функция активации softmax, которая преобразует выход слоя в вероятностное распределение (значения от 0 до 1, сумма которых равна 1). Это удобно для классификации, когда нужно выбрать один из нескольких классов.
# выходной слой получает входной вектор из 10 чисел от предыдущего слоя и преобразует его в вероятностное распределение для 10 классов
#Компилируем модель
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

#Обучаем сеть
model.fit(x_train, y_train, epochs=10, validation_split=0.1)

#Оцениваем качество обучения сети на тестовых данных
scores = model.evaluate (x_test, y_test, verbose=0)
print("Точность работы на тестовых данных: %.2f%%" % (scores[1]*100))

#Создаем последовательную модель №2
model2 = Sequential()
#Добавляем слои
model2.add(Dense( 50, input_dim=784, activation='relu')) #50 нейронов, входной вектор изображения 28х28=784, функция активации relu
model2.add(Dense(10, activation='softmax')) # выходной слой получает входной вектор из 10 чисел от предыдущего слоя и преобразует его в вероятностное распределение для 10 классов
#Компилируем модель
model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

#Обучаем сеть
model2.fit(x_train, y_train, epochs=10, validation_split=0.1)

#Оцениваем качество обучения сети на тестовых данных
scores = model2.evaluate (x_test, y_test, verbose=0)
print("Точность работы на тестовых данных: %.2f%%" % (scores[1]*100))

#Создаем последовательную модель №3
model3 = Sequential()

#Добавляем уровни сети
model3.add(Dense(50, input_dim=784, activation='relu')) #50 нейронов, входной вектор изображения 28х28=784, функция активации relu
model3.add(Dense(50, activation='relu')) #50 нейронов, входной вектор изображения 28х28=784, функция активации relu
model3.add(Dense(10, activation='softmax')) # выходной слой получает входной вектор из 10 чисел от предыдущего слоя и преобразует его в вероятностное распределение для 10 классов

#Компилируем модель
model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

#Обучаем сеть
model3.fit(x_train, y_train, epochs=10, validation_split=0.1)

#Оцениваем качество обучения сети на тестовых данных
scores = model3.evaluate (x_test, y_test, verbose=0)
print("Точность работы на тестовых данных: %.2f%%" % (scores[1]*100))

from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten
import numpy as np

#Загружаем данные
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()
#Преобразуем изображения в формат, подходящий для подачи в сверточную нейронную сеть (с указанием размерности канала)
x_train = x_train[:,:,:,np.newaxis] / 255.0
x_test = x_test[:,:,:,np.newaxis] / 255.0
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

#Создаем последовательную модель №4
model4 = Sequential()
model4.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28,28, 1))) #Добавляем сверточный слой. Число свёрточных фильтров (извлекать признаки из изображений) = 64.
#размер ядра свёртки (2x2)
#размерность выходного изображения совпадает с входным
#функция активации ReLU
#форма входных данных (изображения 28x28 пикселей с 1 каналом,черно-белое изображение)
model4.add(MaxPooling2D(pool_size=2)) #Добавляем слой максимальной подвыборки (max pooling). Этот слой уменьшает размерность изображения (сокращает ширину и высоту в 2 раза).
model4.add(Flatten()) # Преобразуем многомерный выход предыдущего слоя в одномерный вектор, чтобы можно было передать данные в плотный (Dense) слой.
model4.add(Dense(10, activation='softmax')) # выходной слой получает входной вектор из 10 чисел от предыдущего слоя и преобразует его в вероятностное распределение для 10 классов
#Компилируем модель
model4.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
#Обучаем сеть
model4.fit(x_train, y_train, epochs=10, validation_split=0.1)

#Оцениваем качество обучения сети на тестовых данных
scores = model4.evaluate (x_test, y_test, verbose=0)
print("Точность работы на тестовых данных: %.2f%%" % (scores[1]*100))

"""Проведите сравнение работы четырех архитектур. Какая из архитектур
показывает лучший результат и почему? Как можно улучшить результаты
каждой из архитектур?

1. Архитектура model  - Очень простая модель с небольшим количеством параметров. Можно увеличить число нейронов или добавить дополнительные скрытые слои.  
2. Архитектура model2  - Увеличение числа нейронов по сравнению с model позволяет модели лучше улавливать особенности данных. Можно добавить дополнительные скрытые слои.
3. Архитектура model3  - Глубокая архитектура (несколько полносвязных слоёв) позволяет модели представлять более сложные зависимости. Можно применить dropout (техника регуляризации) между слоями для борьбы с переобучением.   
4. Архитектура model4 (CNN)  - Даёт более высокую точность по сравнению с полносвязными нейронными сетями, работающими с плоским входом. Можно увеличить глубину сети, добавив дополнительные сверточные слои.

Реализуйте на питоне алгоритм корректировки синаптических весов с помощью алгоритма обратного распространения ошибки, используя в качестве функции активации логистический сигмоид f(net) = 1\(1+exp(-net))

• Архитектура:
 – 3 входа
 – 2 нейрона скрытого слоя
 – 1 нейрон выходного слоя

• Скорость обучения: η = 0.2  

• Входной вектор:
 X = [0.4, –0.7, 1.3]

• Матрицы весов (с учётом bias):
 Для скрытого слоя (W1) – матрица размером 4×2, где первые 3 строки – веса от входов, а 4-я строка – bias:
  W1 = [ [0.4, –0.7],
      [1.2, 0.6],
      [0.1, 0.5],
      [–1.4, 0.5] ]
 Для выходного слоя (W2) – столбец из 3 элементов, где первые 2 – веса от нейронов скрытого слоя, а 3-й – bias:
  W2 = [ –0.8,
      0.3,
      0.5 ]

• Эталонный (целевой) выход: Y* = 0.7
"""

import numpy as np

# Функция активации
def sigmoid(net):
    return 1 / (1 + np.exp(-net))

# Производная функции активации
def sigmoid_deriv(output):
    # Принимаем на вход значение сигмоида: f(net)
    return output * (1 - output)

learning_rate = 0.2

X = np.array([0.4, -0.7, 1.3]) # Входной вектор (3 признака)

# Добавим смещение = 1 к входному вектору для обработки смещения.
X_ext = np.insert(X, 0, 1)

W1 = np.array([[ 0.4, -0.7],
               [ 1.2,  0.6],
               [ 0.1,  0.5],
               [-1.4,  0.5]])

W2 = np.array([[-0.8],
               [ 0.3],
               [ 0.5]])

target = 0.0

# Прямой проход (forward propagation)

# Вычисление выхода скрытого слоя
# Для каждого нейрона скрытого слоя: net = X_ext • W1 (столбцово)
net_hidden = np.dot(X_ext, W1)       # Вычисляем произведение расширенного входного вектора X_ext на матрицу весов W1
out_hidden = sigmoid(net_hidden)     # Применяем функцию активации ко всем элементам вектора net_hidden

# Добавляем смещение (bias) к выходам скрытого слоя для входа в выходной слой
out_hidden_ext = np.insert(out_hidden, 0, 1)

# Вычисление выхода сети (одного выходного нейрона)
net_output = np.dot(out_hidden_ext, W2)
out_network = sigmoid(net_output)

print("Результаты прямого прохода:")
print("Вход с bias:", X_ext)
print("net скрытого слоя (Произведение):", net_hidden)
print("Выход скрытого слоя (после применения функции активации):", out_hidden)
print("net выходного нейрона:", net_output)
print("Выход сети:", out_network)

# Обратное распространение ошибки (backpropagation)

# Вычисление ошибки (подставляем формулу для квадратичной функции ошибки)
error = target - out_network
# Для выходного нейрона: δ = (t - y) * f'(net)
delta_output = error * sigmoid_deriv(out_network)
print("\nОшибка на выходе:", error)
print("Дельта выходного нейрона:", delta_output)

# Обновление весов второго слоя (W2)
# Новый вес = старый вес + скорость обучения * δ_out * вход в этот нейрон
# Здесь входом является расширенный выход скрытого слоя out_hidden_ext
W2_update = learning_rate * delta_output * out_hidden_ext.reshape(-1, 1)
W2_new = W2 + W2_update
print("\nОбновление весов второго слоя:")
print("DeltaW2:\n", W2_update)
print("Новые веса W2:\n", W2_new)

# Вычисление вклада ошибки в скрытый слой:
# Для каждого нейрона скрытого слоя:
# δ_hidden = f'(net_hidden) * (δ_output * соответствующий вес из W2)
# Заметим, что для W2 первый элемент – смещение, его не используем.
delta_hidden = sigmoid_deriv(out_hidden) * (delta_output * W2[1:].reshape(-1))
print("\nДельты скрытого слоя:", delta_hidden)

# Обновление весов первого слоя (W1)
# Вход для скрытых нейронов – расширенный вход X_ext (размер (4,))
# Для каждого скрытого нейрона j: Δw_ij = learning_rate * δ_hidden[j] * X_ext[i]
# Нужно обновить каждый вес для каждого нейрона скрытого слоя.
# W1 имеет размер (4,2), поэтому обновляем по колонкам.
# Для j-й колонки:
W1_update = np.outer(X_ext, delta_hidden) * learning_rate
W1_new = W1 + W1_update
print("\nОбновление весов первого слоя:")
print("DeltaW1:\n", W1_update)
print("Новые веса W1:\n", W1_new)

# Итоговый вывод обновлённых весов:
print("\nИтоговые обновлённые веса:")
print("W1:\n", W1_new)
print("W2:\n", W2_new)